{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2590fd22-9096-4f87-8412-ad38ff375727",
   "metadata": {},
   "source": [
    "## Load EPA FRS datasets to get facility data and NAICS/SIC mappings\n",
    "\n",
    "Copyright (C) 2021 OS-Climate\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "Contributed by Michael Tiemann (Github: MichaelTiemannOSC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6dc33c-ef3a-4820-a196-1b299d04a5da",
   "metadata": {},
   "source": [
    "Load Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9272c4-2f67-4769-b994-f4f110da425f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# From the AWS Account page, copy the export scripts from the appropriate role using the \"Command Line or Programmatic Access\" link\n",
    "# Paste the copied text into ~/credentials.env\n",
    "\n",
    "from dotenv import dotenv_values, load_dotenv\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "dotenv_dir = os.environ.get('CREDENTIAL_DOTENV_DIR', os.environ.get('PWD', '/opt/app-root/src'))\n",
    "dotenv_path = pathlib.Path(dotenv_dir) / 'credentials.env'\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path=dotenv_path,override=True)\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab4a677-d6b3-4aa7-bffd-52d6c7e90273",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1604f52a-d5dc-424e-8275-8dba4753f7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from osc_ingest_trino import *\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "import io\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2554e5b-cac3-4495-b9f5-2b08da83c6b6",
   "metadata": {},
   "source": [
    "Create an S3 resource for the bucket holding source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f457d9c-cd23-41c4-bdb9-051b401a8951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3_resource = boto3.resource(\n",
    "    service_name=\"s3\",\n",
    "    endpoint_url=os.environ['S3_LANDING_ENDPOINT'],\n",
    "    aws_access_key_id=os.environ['S3_LANDING_ACCESS_KEY'],\n",
    "    aws_secret_access_key=os.environ['S3_LANDING_SECRET_KEY'],\n",
    ")\n",
    "bucket = s3_resource.Bucket(os.environ['S3_LANDING_BUCKET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f7dcf7-352a-4df6-bdba-016a3713773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an S3 client.  We will user later when we write out data and metadata\n",
    "s3 = boto3.client(\n",
    "    service_name=\"s3\",\n",
    "    endpoint_url=os.environ['S3_DEV_ENDPOINT'],\n",
    "    aws_access_key_id=os.environ['S3_DEV_ACCESS_KEY'],\n",
    "    aws_secret_access_key=os.environ['S3_DEV_SECRET_KEY'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d090f71f-b23b-4473-bb9f-b2bebf10cfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trino\n",
    "\n",
    "conn = trino.dbapi.connect(\n",
    "    host=os.environ['TRINO_HOST'],\n",
    "    port=int(os.environ['TRINO_PORT']),\n",
    "    user=os.environ['TRINO_USER'],\n",
    "    http_scheme='https',\n",
    "    auth=trino.auth.JWTAuthentication(os.environ['TRINO_PASSWD']),\n",
    "    verify=True,\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Show available schemas to ensure trino connection is set correctly\n",
    "cur.execute('show schemas in osc_datacommons_dev')\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00406e4-902d-4791-b0c0-632a7918862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_uuid = str(uuid.uuid4())\n",
    "\n",
    "custom_meta_key_fields = 'metafields'\n",
    "custom_meta_key = 'metaset'\n",
    "\n",
    "schemaname = 'epa_frs'\n",
    "cur.execute('create schema if not exists osc_datacommons_dev.' + schemaname)\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4b9d10-90a9-4592-8bd0-0a343921a617",
   "metadata": {},
   "source": [
    "For osc_datacommons_dev, a trino pipeline is a parquet data stored in the S3_DEV_BUCKET\n",
    "It is a 5-step process to get there from a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992c6992-aa20-482e-a4f3-51b6c7277a13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_trino_pipeline (s3, schemaname, tablename, timestamp, df, meta_fields, meta_content):\n",
    "    global ingest_uuid\n",
    "    global custom_meta_key_fields, custom_meta_key\n",
    "    \n",
    "    # First convert dataframe to pyarrow for type conversion and basic metadata\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    # Second, since pyarrow tables are immutable, create a new table with additional combined metadata\n",
    "    if meta_fields or meta_content:\n",
    "        meta_json_fields = json.dumps(meta_fields)\n",
    "        meta_json = json.dumps(meta_content)\n",
    "        existing_meta = table.schema.metadata\n",
    "        combined_meta = {\n",
    "            custom_meta_key_fields.encode(): meta_json_fields.encode(),\n",
    "            custom_meta_key.encode(): meta_json.encode(),\n",
    "            **existing_meta\n",
    "        }\n",
    "        table = table.replace_schema_metadata(combined_meta)\n",
    "    # Third, convert table to parquet format (which cannot be written directly to s3)\n",
    "    pq.write_table(table, '/tmp/{sname}.{tname}.{uuid}.{timestamp}.parquet'.format(sname=schemaname, tname=tablename, uuid=ingest_uuid, timestamp=timestamp))\n",
    "    # df.to_parquet('/tmp/{sname}.{tname}.{uuid}.parquet'.format(sname=schemaname, tname=tablename, uuid=ingest_uuid, index=False))\n",
    "    # Fourth, put the parquet-ified data into our S3 bucket for trino.  We cannot compute parquet format directly to S3 but we can copy it once computed\n",
    "    s3.upload_file(\n",
    "        Bucket=os.environ['S3_DEV_BUCKET'],\n",
    "        Key='trino/{sname}/{tname}/{uuid}/{timestamp}/{tname}.parquet'.format(sname=schemaname, tname=tablename, uuid=ingest_uuid, timestamp=timestamp),\n",
    "        Filename='/tmp/{sname}.{tname}.{uuid}.{timestamp}.parquet'.format(sname=schemaname, tname=tablename, uuid=ingest_uuid, timestamp=timestamp)\n",
    "    )\n",
    "    # Finally, create the trino table backed by our parquet files enhanced by our metadata\n",
    "    cur.execute('.'.join(['drop table if exists osc_datacommons_dev', schemaname, tablename]))\n",
    "    print('dropping table: ' + tablename)\n",
    "    cur.fetchall()\n",
    "    \n",
    "    schema = create_table_schema_pairs(df)\n",
    "\n",
    "    tabledef = \"\"\"create table if not exists osc_datacommons_dev.{sname}.{tname}(\n",
    "{schema}\n",
    ") with (\n",
    "    format = 'parquet',\n",
    "    external_location = 's3a://{bucket}/trino/{sname}/{tname}/{uuid}/{timestamp}'\n",
    ")\"\"\".format(schema=schema,bucket=os.environ['S3_DEV_BUCKET'],sname=schemaname,tname=tablename,uuid=ingest_uuid,timestamp=timestamp)\n",
    "    print(tabledef)\n",
    "\n",
    "    # tables created externally may not show up immediately in cloud-beaver\n",
    "    cur.execute(tabledef)\n",
    "    cur.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4544e38-c59b-4a49-907f-83db1e0bc16b",
   "metadata": {},
   "source": [
    "Load EPA FRS data file using pandas *read_csv* and using *ingest_uuid* as the global UUID for this ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5291b1af-eb98-4a25-adf2-74ce7b2889c0",
   "metadata": {},
   "source": [
    "Construct the combined metadata by merging existing table metadata and custom metadata.\n",
    "Note: The metadata content must be JSON serialisable and encoded as bytes; the metadata key must also be encoded as bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb94080c-3bce-4b37-8c91-e83ab46c7b10",
   "metadata": {},
   "source": [
    "The schemaname for this table is `epa_frs`.  Dunno if it's a good idea or simply redundant to prefix the tablename with `epa_frs_` or `epa_` or `frs_` or..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f30783-f9ca-4ea2-b765-ea9d8350c616",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_meta_content = {\n",
    "    'title': 'EPA FRS Dataset',\n",
    "    'author': 'US EPA',\n",
    "    'contact': 'frs_support@epa.gov',\n",
    "    'description': 'desc',\n",
    "    'release_date': '20211104',\n",
    "    # How should we describe our transformative step here?\n",
    "}\n",
    "\n",
    "naics_metadata_df = None\n",
    "custom_meta_fields = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4359751-9eac-4847-9865-e39aab1405a7",
   "metadata": {},
   "source": [
    "Prepare GLEIF matching data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935dc322-6391-47fa-96bc-db924178ee4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gleif_file = s3_resource.Object(os.environ['S3_LANDING_BUCKET'],f'mtiemann-GLEIF/EPA_FRS-matches.csv')\n",
    "gleif_file.download_file(f'/tmp/EPA_FRS-gleif.csv')\n",
    "gleif_df = pd.read_csv(f'/tmp/EPA_FRS-gleif.csv', header=0, sep=',', dtype=str, engine='c')\n",
    "gleif_dict = { k:v for k, v in zip(gleif_df['ORG_NAME'], gleif_df.LEI) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54497409-7ef1-4aff-b07b-23ef754e6617",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_date_parser = lambda x: x # pd.to_datetime (x, format='%d-%m-%y')\n",
    "\n",
    "usecols_dict = {\n",
    "    'ORGANIZATION': [\"REGISTRY_ID\",\"PGM_SYS_ACRNM\",\"INTEREST_TYPE\",\"AFFILIATION_TYPE\",\"START_DATE\",\"END_DATE\",\"ORG_NAME\",\"ORG_TYPE\",\"DUNS_NUMBER\",\"DIVISION_NAME\",\"EIN\",\"MAILING_ADDRESS\",\"SUPPLEMENTAL_ADDRESS\",\"CITY_NAME\",\"STATE_CODE\",\"POSTAL_CODE\",\"COUNTRY_NAME\"],\n",
    "    'FACILITY': [\"REGISTRY_ID\",\"PRIMARY_NAME\",\"LOCATION_ADDRESS\",\"CITY_NAME\",\"COUNTY_NAME\",\"STATE_CODE\",\"COUNTRY_NAME\",\"POSTAL_CODE\",\"HUC_CODE\",\"PGM_SYS_ACRNMS\",\"LATITUDE83\",\"LONGITUDE83\"],\n",
    "    'NAICS': [\"REGISTRY_ID\",\"PGM_SYS_ACRNM\",\"INTEREST_TYPE\",\"NAICS_CODE\",\"PRIMARY_INDICATOR\",\"CODE_DESCRIPTION\"],\n",
    "    'SIC': [\"REGISTRY_ID\",\"PGM_SYS_ACRNM\",\"INTEREST_TYPE\",\"SIC_CODE\",\"PRIMARY_INDICATOR\",\"CODE_DESCRIPTION\"],\n",
    "}\n",
    "\n",
    "timestamp = ''\n",
    "for name in usecols_dict.keys():\n",
    "    bObj = bucket.Object(f'EPA/national_combined-20211104/NATIONAL_{name}_FILE.CSV')\n",
    "    timestamp = max(bObj.last_modified.isoformat(), timestamp)\n",
    "    \n",
    "for name in usecols_dict.keys():\n",
    "    bObj = bucket.Object(f'EPA/national_combined-20211104/NATIONAL_{name}_FILE.CSV')\n",
    "    bObj.download_file(f'/tmp/foo{timestamp}.csv')\n",
    "\n",
    "    # It takes almost a minute (!) to load nearly 600K rows of data\n",
    "    dtype = { col:\"string\" for col in usecols_dict[name] }\n",
    "    if name == 'ORGANIZATION':\n",
    "        parse_dates=['START_DATE', 'END_DATE']\n",
    "    else:\n",
    "        parse_dates = None\n",
    "        if name == 'FACILITY':\n",
    "            dtype[\"LATITUDE83\"] = dtype[\"LONGITUDE83\"] = 'float32'\n",
    "    \n",
    "    df = pd.read_csv(f'/tmp/foo{timestamp}.csv', sep=',', header=0, usecols=usecols_dict[name], dtype=dtype, parse_dates=parse_dates, date_parser=custom_date_parser, engine='c')\n",
    "    os.unlink(f'/tmp/foo{timestamp}.csv')\n",
    "    if name == 'ORGANIZATION':\n",
    "        df['LEI'] = df['ORG_NAME'].map(gleif_dict)\n",
    "        del(gleif_dict)\n",
    "        df.EIN.apply(lambda x: x if pd.isna(x) else x.replace('-', ''))\n",
    "        df.loc[(df.EIN>'999999999')|(df.EIN.str.len()!=9), 'EIN'] = '-1'\n",
    "        df.EIN = df.EIN.astype('Int64')\n",
    "        cols = df.columns.tolist()\n",
    "        cols = [ cols[0], cols[-1]] + cols[1:-1]\n",
    "        df = df[cols]\n",
    "    df.info(verbose=True)\n",
    "    tablename = name.lower()\n",
    "    create_trino_pipeline (s3, schemaname, tablename, timestamp, df, custom_meta_fields, custom_meta_content)\n",
    "    del(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a88136-839c-4187-bd75-8a1da7da343b",
   "metadata": {},
   "source": [
    "Verify that we can restore data and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27089453-6573-497b-a096-2c94afcc2a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Parquet file into an Arrow table\n",
    "obj = s3.get_object(\n",
    "    Bucket=os.environ['S3_DEV_BUCKET'], \n",
    "    Key='trino/{sname}/{tname}/{uuid}/{timestamp}/{tname}.parquet'.format(sname=schemaname, tname=tablename, uuid=ingest_uuid, timestamp=timestamp)\n",
    ")\n",
    "restored_table = pq.read_table(io.BytesIO(obj['Body'].read()))\n",
    "# Call the table’s to_pandas conversion method to restore the dataframe\n",
    "# This operation uses the Pandas metadata to reconstruct the dataFrame under the hood\n",
    "restored_df = restored_table.to_pandas()\n",
    "# The custom metadata is accessible via the Arrow table’s metadata object\n",
    "# Use the custom metadata key used earlier (taking care to once again encode the key as bytes)\n",
    "restored_meta_json = restored_table.schema.metadata[custom_meta_key.encode()]\n",
    "# Deserialize the json string to get back metadata\n",
    "restored_meta = json.loads(restored_meta_json)\n",
    "# Use the custom metadata fields key used earlier (taking care to once again encode the key as bytes)\n",
    "restored_meta_json_fields = restored_table.schema.metadata[custom_meta_key_fields.encode()]\n",
    "# Deserialize the json string to get back metadata\n",
    "restored_meta_fields = json.loads(restored_meta_json_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8e9216-403f-481b-851d-3123f7c20143",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "restored_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1687e7b0-8ecd-4545-a7f0-a695540273f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "restored_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493c3f2d-8b61-4cf8-a2b8-fc831d119e2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "restored_meta_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ec80d1-600a-4e0b-bb8b-2b2c8ca30f50",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load metadata following an ingestion process into trino metadata store\n",
    "\n",
    "### The schema is *metastore*, and the table names are *meta_schema*, *meta_table*, *meta_field*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7988f89c-318a-40f2-a734-3fb791bfd69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metastore structure\n",
    "metastore = {'catalog':'osc_datacommons_dev',\n",
    "             'schema':'epa_frs',\n",
    "             'table':tablename,\n",
    "             'metadata':custom_meta_content,\n",
    "             'uuid':ingest_uuid}\n",
    "# Create DataFrame\n",
    "df_meta = pd.DataFrame(metastore)\n",
    "# Print the output\n",
    "df_meta\n",
    "\n",
    "# ??? Now what?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcaf595-2877-46f1-86c4-37a83efd6e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(f'select name from osc_datacommons_dev.sec_dera.sub where osc_datacommons_dev.sec_dera.sub.ein in (select distinct(osc_datacommons_dev.{schemaname}.organization.ein) from osc_datacommons_dev.{schemaname}.organization where EIN > 0)')\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc18014-916a-486e-bac8-71dc6c5eb578",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(f'select name from osc_datacommons_dev.sec_dera.sub where osc_datacommons_dev.sec_dera.sub.lei in (select distinct(osc_datacommons_dev.{schemaname}.organization.lei) from osc_datacommons_dev.{schemaname}.organization where lei is not null)')\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74387444-e5e8-4fa2-97c0-76ecfefb1af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(f'select name from osc_datacommons_dev.sec_dera.sub where osc_datacommons_dev.sec_dera.sub.name in (select distinct(osc_datacommons_dev.{schemaname}.organization.org_name) from osc_datacommons_dev.{schemaname}.organization)')\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab3d7fb-e049-4a3f-9c8e-dfb60f45b625",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(f'select distinct(osc_datacommons_dev.{schemaname}.organization.org_name) from osc_datacommons_dev.{schemaname}.organization where LEI is not null')\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f734b3-fce4-425e-93fb-a8324c6b7bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(f'select distinct(osc_datacommons_dev.sec_dera.sub.name) from osc_datacommons_dev.sec_dera.sub where LEI is not null')\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6213d62d-8ca1-403e-a871-f8211f7af4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute ('select count (*) from osc_datacommons_dev.epa_frs.organization where LEI is not null')\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bd96f3-94a6-4f0f-8dd9-961e9e2f0380",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute ('select count (*) from osc_datacommons_dev.sec_dera.sub where LEI is not null')\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c01ba6-6051-4f02-8311-5ab63884e1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute ('select count (*) from osc_datacommons_dev.epa_ghgrp.parent_company where LEI is not null')\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9655f425-ca03-4257-b515-3afca3280d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute ('select count (*) from (select distinct (lei) from osc_datacommons_dev.epa_ghgrp.parent_company where LEI is not null)')\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaef5a6-a442-44ac-baf9-48d68955fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute ('select count (*) from osc_datacommons_dev.epa_frs.organization where LEI is not null')\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16aaad7-6a9c-4e97-986b-c2f25fddea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute ('select count (*) from (select distinct (lei) from osc_datacommons_dev.epa_frs.organization where LEI is not null)')\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09490a4b-884b-4052-a744-c1556441d13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute ('select count (*) from osc_datacommons_dev.us_census.all_sector_survey_2017')\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b66729-4ca3-48bb-af09-b1dcc67ceb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute ('show schemas in osc_datacommons_dev')\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf73aef-9361-4546-a456-94937971de40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
